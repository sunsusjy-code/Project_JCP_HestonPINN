import torch
import torch.optim as optim
from loss import CausalLoss  # å¯¼å…¥æˆ‘ä»¬åœ¨ Step 3 å†™å¥½çš„ç±»

def train(model, config, data_generator):
    # 1. åˆå§‹åŒ–ä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=config['training']['lr'])
    
    # 2. åˆå§‹åŒ– Loss è®¡ç®—å™¨ (Heston PDE å°±åœ¨è¿™é‡Œé¢)
    criterion = CausalLoss(config)
    
    # 3. è¯»å– Epochs
    epochs = config['training']['epochs']
    
    print(f"ğŸš€ Start Training Heston Model...")
    print(f"âš™ï¸  Config: Epsilon={config['training']['epsilon']} (Should be 0.0 for Baseline)")

    # --- Training Loop ---
    model.train()
    for epoch in range(epochs):
        optimizer.zero_grad()
        
        # A. ä» DataGenerator è·å– 3D æ•°æ®
        # å†…éƒ¨ç‚¹ (S, v, t)
        domain_points = data_generator.get_interior_points()
        
        # åˆå§‹æ¡ä»¶ (S, v, 0) å’Œ Payoff
        ic_points, ic_val = data_generator.get_initial_condition_points()
        
        # è¾¹ç•Œæ¡ä»¶ (S=0, S=max)
        boundary_batch = data_generator.get_boundary_points()
        
        # B. è®¡ç®— Loss (è°ƒç”¨ CausalLoss çš„ forward)
        # æ³¨æ„ï¼šè¿™é‡Œä¼šè‡ªåŠ¨æ ¹æ® epsilon=0 é€€åŒ–ä¸º Standard Loss
        total_loss, loss_pde, loss_ic, mean_w = criterion(
            model, domain_points, ic_points, ic_val, boundary_batch
        )
        
        # C. åå‘ä¼ æ’­
        total_loss.backward()
        optimizer.step()

        # D. æ‰“å°æ—¥å¿—
        if epoch % 100 == 0:
            print(f"Epoch {epoch:5d} | Total: {total_loss.item():.6f} | "
                  f"PDE: {loss_pde.item():.6f} | IC: {loss_ic.item():.6f} | "
                  f"Mean W: {mean_w.item():.4f}")

    print("âœ… Training Finished.")